# Future Directions — Multi‑Agent Engineering System

> This document captures *future-facing ideas* and intentional non-goals for the multi-agent engineering project. It is explicitly **not** a backlog to execute immediately. The purpose is to preserve thinking without forcing premature design or implementation.

---

## 1. Core Trajectory (Already in Motion)

### 1.1 Spec → App as a First-Class Capability

**Goal**  
The system accepts a written specification and produces a runnable software skeleton or application.

**Key characteristics**
- Spec is the *single source of intent*
- Agents collaborate through **structured artifacts**, not shared memory
- Pipeline enforces:
  - scope discipline
  - explicit ownership
  - verifiable outputs

**Status**  
This is already partially implemented and validated. Further work here is refinement, not discovery.

---

## 2. Artifact Lineage & Traceability (High-Leverage, High-Complexity)

> *This is a second-order system built **on top of** the core pipeline.*

### 2.1 Visual Artifact Tracing

**Idea**  
Make it possible to visually inspect:
- which agent produced which artifact
- from which inputs
- using which model
- under which prompt + constraints

Think:
- timeline / graph view
- artifacts as nodes
- agent + model metadata as edges

**Why this matters**
- Debugging agentic systems is otherwise opaque
- Enables learning-by-comparison across runs
- Turns LLM behavior into something inspectable, not mystical

**Explicit complexity warning**  
This is *not* just a UI problem. It requires:
- consistent metadata contracts
- stable run identifiers
- careful separation of content vs provenance

This should not be attempted until the core pipeline is boringly reliable.

---

## 3. Comparative Runs & Diffing (Emergent, Not Yet Designed)

### 3.1 Cross-Run Comparison

**Rough idea**
- Run the same spec multiple times
- Vary:
  - model
  - temperature / settings
  - agent prompts
- Compare:
  - resulting artifacts
  - plans
  - file manifests

This is *not* traditional diffing. It’s semantic and structural.

**Open questions (intentionally unanswered)**
- What is the unit of comparison?
- Files? Plans? Decisions?
- How do you avoid Goodhart-style over-metrics?

This space should remain speculative for now.

---

## 4. Agent Roles as Replaceable Modules

### 4.1 Agent Swappability

Future direction:
- Treat agents as interchangeable strategies
- Example:
  - different "Engineering Lead" personas
  - different testing philosophies

This enables:
- controlled experiments
- pedagogy
- stress-testing assumptions

But it also multiplies surface area. Deferred intentionally.

---

## 5. Non-Goals (For Now)

These are explicitly **out of scope** for the foreseeable future:
- Self-modifying agents
- Long-term memory across runs
- Autonomous project continuation
- "Fully automatic" software generation without human checkpoints

The system is meant to be *inspectable, interruptible, and educational* — not autonomous.

---

## 6. The Missing Thought (Likely Candidates)

Based on prior discussions, the thought you forgot may have been one of these:

- **Human-in-the-loop phase gates** as a first-class design element
- **Failure capture as a feature** (failed runs are artifacts, not waste)
- **Pedagogical mode**: making the system explain *why* decisions were made
- **Run journals**: narrative summaries stitched from artifacts

None of these need to be decided now.

---

## 7. Closing Note

This project has already crossed the threshold from *exercise* to *system*.  
Future work should prioritize:
- clarity over cleverness
- containment over capability
- learning over automation

If it feels heavy: that’s a signal to stop — not to simplify prematurely.

